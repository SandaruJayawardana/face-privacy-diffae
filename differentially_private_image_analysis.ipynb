{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib.util\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from simpleinfotheory import entropy\n",
    "\n",
    "# from dp_laplace_mechanism import laplace_mechanism\n",
    " \n",
    "# specify the module that needs to be\n",
    "# imported relative to the path of the\n",
    "# module\n",
    "latent_load_module = importlib.util.spec_from_file_location(\"load_latent\",\n",
    "                                                            \"/home/sjay9734/diff_encoder/face_privacy/face-privacy-diffae/utils/load_latent.py\")\n",
    "latent_load = importlib.util.module_from_spec(latent_load_module)\n",
    "latent_load_module.loader.exec_module(latent_load)\n",
    "\n",
    "X_train, y_train, X_test, y_test = latent_load.pre_process_celebA(TRAINING_AMOUNT = 0.9)\n",
    "\n",
    "laplace_mechanism_loader = importlib.util.spec_from_file_location(\"load_latent\",\n",
    "                                                                  \"/home/sjay9734/diff_encoder/face_privacy/face-privacy-diffae/privacy_mechanisms/dp_laplace_mechanism.py\")\n",
    "laplace = importlib.util.module_from_spec(laplace_mechanism_loader)\n",
    "laplace_mechanism_loader.loader.exec_module(laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train inference model (MLP Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "\n",
    "LATENT_SIZE = 512\n",
    "MAX_VARIANCE = 7\n",
    "\n",
    "def create_dataset():\n",
    "    i = 0\n",
    "    train_X_list = []\n",
    "    train_y_list = []\n",
    "\n",
    "    for i in range(LATENT_SIZE):\n",
    "        train_y_list.append([])\n",
    "    # print((train_y_list))\n",
    "    i = 0\n",
    "    while i < (60000):\n",
    "        \n",
    "        if np.random.randint(2, size=1) == 1:\n",
    "            train_X_list.append(X_train[i])\n",
    "            for axis in range(LATENT_SIZE):\n",
    "                # print(X_train[i, axis], X_train[i][axis])\n",
    "                train_y_list[axis].append(float(X_train[i, axis]))\n",
    "            i += 1\n",
    "        else:\n",
    "            random_index = np.random.randint(len(X_train), size=1)\n",
    "            gaussian_noise = np.random.normal(loc=0.0, \n",
    "                                            scale=np.random.randint(MAX_VARIANCE, size=1), size=LATENT_SIZE)\n",
    "            train_X_list.append(np.reshape(X_train[random_index] + gaussian_noise, (LATENT_SIZE)))\n",
    "            for axis in range(LATENT_SIZE):\n",
    "                # print(train_y_list[axis], axis, np.shape(X_train), (X_train[random_index, axis]))\n",
    "                train_y_list[axis].append(float(X_train[random_index, axis]))\n",
    "            \n",
    "    augmented_train_X = np.array(train_X_list)\n",
    "    augmented_train_y = np.array(train_y_list)\n",
    "\n",
    "    return augmented_train_X, augmented_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model_list = []\n",
    "input_data, target_data_ = create_dataset()\n",
    "\n",
    "print(np.shape(input_data), np.shape(target_data_))\n",
    "\n",
    "for i in range(LATENT_SIZE):\n",
    "    # print(len(target_data_[i]))\n",
    "    target_data = target_data_[1]\n",
    "    \n",
    "    model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=10000)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(input_data, target_data)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    # y_pred = model.predict(X_test)\n",
    "    model_list.append(model)\n",
    "    print(\"Axis \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,model_ in enumerate(model_list):\n",
    "#     with open(f'mlp_models_new/mlp_regression_model_64_32_200000_{i}.pkl', 'wb') as file:\n",
    "#         pickle.dump(model_, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_list = []\n",
    "for i in range(LATENT_SIZE):\n",
    "    with open(f'mlp_models_new/mlp_regression_model_64_32_200000_{i}.pkl', 'rb') as file:\n",
    "        loaded_model_list.append(pickle.load(file))\n",
    "\n",
    "model_list = loaded_model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_original_latent(perturbed_latent, filtered_list = []):\n",
    "    guessed_latent = np.array(perturbed_latent)\n",
    "    if len(filtered_list) > 0:\n",
    "        for i in filtered_list:\n",
    "            guessed_latent[i] = model_list[i].predict(np.reshape(perturbed_latent, (1,-2)))\n",
    "    else:\n",
    "        for i in range(LATENT_SIZE):\n",
    "            guessed_latent[i] = model_list[i].predict(np.reshape(perturbed_latent, (1,-2)))\n",
    "    \n",
    "    return (guessed_latent+perturbed_latent) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_arr = []\n",
    "\n",
    "for i in range(512):\n",
    "    sensitivity_arr.append(laplace.local_sensitivity_and_min_max(X_train[:,i], 0.1))\n",
    "\n",
    "laplace_construct = laplace.laplace_mechanism(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP with laplace\n",
    "\n",
    "def dp_latent(latent, eps):\n",
    "    perturbed_latent = np.zeros(512)\n",
    "    for i in range(512):\n",
    "        perturbed_latent[i] = laplace_construct.gen_random_output(latent[i], eps, \n",
    "                                                                  sensitivity_arr[i][0], \n",
    "                                                                  sensitivity_arr[i][1], \n",
    "                                                                  sensitivity_arr[i][2])\n",
    "    \n",
    "    return perturbed_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 10 # 0.1 -> mse > 12\n",
    "latent_original = 0\n",
    "for i in range(10):\n",
    "    latent_original = X_train[i]\n",
    "    perturbed_latent = dp_latent(X_train[i], EPS)\n",
    "    guess_latent = guess_original_latent(perturbed_latent)\n",
    "\n",
    "    perturbed_l2 = np.linalg.norm(perturbed_latent - X_test[i])\n",
    "    guessed_l2 = np.linalg.norm(guess_latent - X_test[i])\n",
    "\n",
    "    print(\"perturbed_l2 \", perturbed_l2, \" guessed_l2 \", guessed_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Diffusion encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import *\n",
    "from templates_cls import *\n",
    "from experiment_classifier import ClsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "conf = ffhq256_autoenc()\n",
    "# print(conf.name)\n",
    "model = LitModel(conf)\n",
    "state = torch.load(f'/home/sjay9734/diff_encoder/face_privacy/face-privacy-diffae/checkpoints/{conf.name}/last.ckpt', map_location='cpu')\n",
    "model.load_state_dict(state['state_dict'], strict=False)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device);\n",
    "\n",
    "cls_conf = ffhq256_autoenc_cls()\n",
    "cls_model = ClsModel(cls_conf)\n",
    "state = torch.load(f'/home/sjay9734/diff_encoder/face_privacy/face-privacy-diffae/checkpoints/{cls_conf.name}/last.ckpt',\n",
    "                    map_location='cpu')\n",
    "print('latent step:', state['global_step'])\n",
    "cls_model.load_state_dict(state['state_dict'], strict=False);\n",
    "cls_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"imgs_align\"\n",
    "\n",
    "data = ImageDataset(IMAGE_PATH, image_size=conf.img_size, exts=['jpg', 'JPG', 'png'], do_augment=False) # celebA_hq/image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index = 0\n",
    "\n",
    "batch = data[img_index]['img'][None]\n",
    "cond = model.encode(data[img_index]['img'][None].to(device))\n",
    "xT = model.encode_stochastic(batch.to(device), cond, T=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ori = (batch + 1) / 2\n",
    "ax[0].imshow(ori[0].permute(1, 2, 0).cpu())\n",
    "ax[1].imshow(xT[0].permute(1, 2, 0).cpu())\n",
    "plt.imsave(\"perturbed_imgs/1.png\", ori[0].permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
